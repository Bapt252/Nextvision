"""
üöÄ Nextvision V3.0 - Configuration Pytest Centralis√©e
=====================================================

Configuration centrale pour tous les tests Nextvision V3.0.
Optimis√©e pour couverture de code et √©limination des warnings.

üéØ FONCTIONNALIT√âS :
- Configuration environnement de test
- Fixtures communes r√©utilisables
- Gestion des warnings et logging
- Mocks intelligents pour API externes
- Nettoyage automatique apr√®s tests

üìä COUVERTURE :
- Support modules r√©els + fallback mocks
- M√©triques de performance int√©gr√©es
- Rapports d√©taill√©s d'ex√©cution

Author: NEXTEN Team - Coverage Fix
Version: 3.0.0 - Centralized Configuration
"""

import pytest
import asyncio
import os
import sys
import logging
import warnings
import tempfile
import shutil
from pathlib import Path
from typing import Dict, Any, Generator
from unittest.mock import Mock, patch

# Configuration logging pour tests
logging.getLogger().setLevel(logging.WARNING)

# ============================================================================
# CONFIGURATION ENVIRONNEMENT DE TEST
# ============================================================================

def pytest_configure(config):
    """Configuration globale pytest au d√©marrage"""
    
    # Variables d'environnement de test
    os.environ.update({
        'NEXTVISION_ENV': 'test',
        'NEXTVISION_DEBUG': 'false',
        'NEXTVISION_CACHE_ENABLED': 'true',
        'NEXTVISION_LOG_LEVEL': 'WARNING',
        'NEXTVISION_OPENAI_API_KEY': 'test_key_mock',
        'NEXTVISION_GOOGLE_MAPS_API_KEY': 'test_key_mock',
        'PYTEST_RUNNING': 'true',
        'PYTHONDONTWRITEBYTECODE': '1'
    })
    
    # Suppression warnings non critiques
    warnings.filterwarnings("ignore", category=DeprecationWarning)
    warnings.filterwarnings("ignore", category=PendingDeprecationWarning)
    warnings.filterwarnings("ignore", category=UserWarning, module="nextvision.*")
    warnings.filterwarnings("ignore", message=".*validator.*")
    warnings.filterwarnings("ignore", message=".*Field.*")
    warnings.filterwarnings("ignore", message=".*json_encoders.*")
    
    # Configuration logging
    logging.basicConfig(
        level=logging.WARNING,
        format='%(asctime)s [TEST] %(levelname)s: %(message)s',
        handlers=[logging.StreamHandler()]
    )
    
    print("\nüöÄ Nextvision V3.0 - Tests Configuration")
    print("=" * 50)
    print(f"üìÇ R√©pertoire de travail: {os.getcwd()}")
    print(f"üêç Python: {sys.version.split()[0]}")
    print(f"üìã Environnement: {os.environ.get('NEXTVISION_ENV', 'unknown')}")
    print("=" * 50)

def pytest_unconfigure(config):
    """Nettoyage √† la fin des tests"""
    print("\nüßπ Nettoyage final des tests Nextvision V3.0")

# ============================================================================
# FIXTURES GLOBALES - DONN√âES DE TEST
# ============================================================================

@pytest.fixture(scope="session")
def test_data_dir():
    """R√©pertoire temporaire pour donn√©es de test"""
    temp_dir = tempfile.mkdtemp(prefix="nextvision_test_")
    yield Path(temp_dir)
    shutil.rmtree(temp_dir, ignore_errors=True)

@pytest.fixture(scope="session")
def sample_candidate_data() -> Dict[str, Any]:
    """Donn√©es candidat type pour tests"""
    return {
        "id": "candidate_001",
        "name": "Jean Dupont",
        "email": "jean.dupont@example.com",
        "location": "Paris, France",
        "coordinates": {"lat": 48.8566, "lng": 2.3522},
        "skills": ["Python", "Machine Learning", "API Development"],
        "experience_years": 5,
        "motivations": ["salaire", "evolution", "formation"],
        "transport_preferences": ["metro", "velo"],
        "max_commute_time": 45,
        "availability": "immediate",
        "salary_expectation": 55000,
        "contract_type": "CDI"
    }

@pytest.fixture(scope="session")
def sample_job_data() -> Dict[str, Any]:
    """Donn√©es job type pour tests"""
    return {
        "id": "job_001",
        "title": "D√©veloppeur Python Senior",
        "company": "TechCorp",
        "location": "Paris 9√®me, France",
        "coordinates": {"lat": 48.8728, "lng": 2.3386},
        "required_skills": ["Python", "API", "PostgreSQL"],
        "min_experience_years": 3,
        "max_experience_years": 8,
        "salary_range": {"min": 50000, "max": 65000},
        "benefits": ["salaire", "formation", "remote"],
        "contract_type": "CDI",
        "transport_access": ["metro_line_9", "bus"],
        "work_mode": "hybrid"
    }

@pytest.fixture(scope="function")
def candidate_data(sample_candidate_data):
    """Copie modifiable des donn√©es candidat"""
    return sample_candidate_data.copy()

@pytest.fixture(scope="function") 
def job_data(sample_job_data):
    """Copie modifiable des donn√©es job"""
    return sample_job_data.copy()

# ============================================================================
# FIXTURES MOCKS - API EXTERNES
# ============================================================================

@pytest.fixture(scope="function")
def mock_openai_api():
    """Mock pour API OpenAI"""
    with patch('openai.ChatCompletion.create') as mock_create:
        mock_create.return_value = Mock(
            choices=[Mock(
                message=Mock(
                    content='{"score": 0.75, "analysis": "Test analysis"}'
                )
            )],
            usage=Mock(total_tokens=100)
        )
        yield mock_create

@pytest.fixture(scope="function")
def mock_google_maps_api():
    """Mock pour API Google Maps"""
    mock_response = Mock()
    mock_response.json.return_value = {
        "results": [{
            "geometry": {
                "location": {"lat": 48.8566, "lng": 2.3522}
            },
            "formatted_address": "Paris, France"
        }],
        "status": "OK"
    }
    
    with patch('requests.get', return_value=mock_response) as mock_get:
        yield mock_get

@pytest.fixture(scope="function")
def mock_external_apis(mock_openai_api, mock_google_maps_api):
    """Mock combin√© pour toutes les APIs externes"""
    return {
        'openai': mock_openai_api,
        'google_maps': mock_google_maps_api
    }

# ============================================================================
# FIXTURES PERFORMANCE & TIMING
# ============================================================================

@pytest.fixture(scope="function")
def performance_tracker():
    """Tracker de performance pour tests"""
    class PerformanceTracker:
        def __init__(self):
            self.timings = {}
            self.start_times = {}
        
        def start_timer(self, name: str):
            import time
            self.start_times[name] = time.time()
        
        def stop_timer(self, name: str) -> float:
            import time
            if name in self.start_times:
                elapsed = (time.time() - self.start_times[name]) * 1000
                self.timings[name] = elapsed
                return elapsed
            return 0.0
        
        def get_timing(self, name: str) -> float:
            return self.timings.get(name, 0.0)
        
        def assert_timing(self, name: str, max_ms: float):
            timing = self.get_timing(name)
            assert timing <= max_ms, f"{name} took {timing:.1f}ms > {max_ms}ms limit"
    
    return PerformanceTracker()

# ============================================================================
# FIXTURES NEXTVISION SERVICES
# ============================================================================

@pytest.fixture(scope="function")
def nextvision_scorer():
    """Fixture pour scorer Nextvision avec fallback intelligent"""
    try:
        # Tentative d'import du vrai module
        from nextvision.services.enhanced_bidirectional_scorer_v3 import EnhancedBidirectionalScorerV3
        scorer = EnhancedBidirectionalScorerV3(enable_cache=False)
        scorer._test_mode = True  # Flag pour mode test
        return scorer
    except ImportError:
        # Fallback vers mock
        mock_scorer = Mock()
        mock_scorer.calculate_bidirectional_score.return_value = Mock(
            total_score=0.75,
            subscores={'test': 0.75},
            processing_time_ms=50.0,
            timestamp=1234567890.0
        )
        mock_scorer.get_performance_stats.return_value = {
            'total_calculations': 1,
            'average_processing_time_ms': 50.0
        }
        mock_scorer._test_mode = True
        return mock_scorer

@pytest.fixture(scope="function")
def nextvision_transport_calculator():
    """Fixture pour calculateur de transport avec fallback"""
    try:
        from nextvision.services.transport_calculator import TransportCalculator
        calculator = TransportCalculator()
        return calculator
    except ImportError:
        mock_calculator = Mock()
        mock_calculator.calculate_transport_score.return_value = 0.80
        mock_calculator.get_commute_time.return_value = 35
        return mock_calculator

@pytest.fixture(scope="function")
def nextvision_google_maps():
    """Fixture pour service Google Maps avec fallback"""
    try:
        from nextvision.services.google_maps_service import GoogleMapsService
        service = GoogleMapsService(api_key="test_key")
        return service
    except ImportError:
        mock_service = Mock()
        mock_service.geocode.return_value = {
            "lat": 48.8566, "lng": 2.3522
        }
        mock_service.get_distance.return_value = {
            "distance": 1500, "duration": 25
        }
        return mock_service

# ============================================================================
# FIXTURES POUR TESTS D'INT√âGRATION
# ============================================================================

@pytest.fixture(scope="function")
def integration_environment(
    candidate_data, 
    job_data, 
    mock_external_apis,
    nextvision_scorer
):
    """Environnement complet pour tests d'int√©gration"""
    return {
        'candidate': candidate_data,
        'job': job_data,
        'apis': mock_external_apis,
        'scorer': nextvision_scorer,
        'config': {
            'timeout_ms': 1000,
            'enable_cache': False,
            'log_level': 'WARNING'
        }
    }

# ============================================================================
# HOOKS PYTEST - PERSONNALISATION
# ============================================================================

def pytest_runtest_setup(item):
    """Setup avant chaque test"""
    # Marquer le d√©but de test pour m√©triques
    item._test_start_time = None

def pytest_runtest_call(item):
    """Pendant l'ex√©cution du test"""
    import time
    item._test_start_time = time.time()

def pytest_runtest_teardown(item):
    """Nettoyage apr√®s chaque test"""
    if hasattr(item, '_test_start_time') and item._test_start_time:
        import time
        duration = (time.time() - item._test_start_time) * 1000
        
        # Log si test lent
        if duration > 1000:  # > 1 seconde
            print(f"\n‚ö†Ô∏è Test lent: {item.name} ({duration:.1f}ms)")

def pytest_terminal_summary(terminalreporter, exitstatus, config):
    """R√©sum√© final des tests"""
    if hasattr(terminalreporter, 'stats'):
        passed = len(terminalreporter.stats.get('passed', []))
        failed = len(terminalreporter.stats.get('failed', []))
        skipped = len(terminalreporter.stats.get('skipped', []))
        
        print(f"\nüéØ R√âSUM√â TESTS NEXTVISION V3.0:")
        print(f"   ‚úÖ R√©ussis: {passed}")
        print(f"   ‚ùå √âchou√©s: {failed}")
        print(f"   ‚è≠Ô∏è Ignor√©s: {skipped}")
        
        if failed == 0 and passed > 0:
            print(f"üéâ Tous les tests sont pass√©s avec succ√®s !")

# ============================================================================
# HELPERS POUR TESTS
# ============================================================================

def create_test_questionnaire(**kwargs):
    """Helper pour cr√©er questionnaire de test"""
    default_data = {
        "moyens_transport": ["metro", "bus"],
        "temps_max": 45,
        "priorites": ["salaire", "evolution"],
        "disponibilite": "immediate"
    }
    default_data.update(kwargs)
    return default_data

def assert_score_valid(score: float, min_score: float = 0.0, max_score: float = 1.0):
    """Helper pour valider un score"""
    assert isinstance(score, (int, float)), f"Score doit √™tre num√©rique, re√ßu {type(score)}"
    assert min_score <= score <= max_score, f"Score {score} hors limites [{min_score}, {max_score}]"

def assert_processing_time_valid(time_ms: float, max_time_ms: float = 175.0):
    """Helper pour valider temps de traitement"""
    assert isinstance(time_ms, (int, float)), f"Temps doit √™tre num√©rique, re√ßu {type(time_ms)}"
    assert time_ms >= 0, f"Temps n√©gatif: {time_ms}ms"
    assert time_ms <= max_time_ms, f"Temps {time_ms}ms > limite {max_time_ms}ms"

# ============================================================================
# CONFIGURATION ASYNCIO POUR TESTS ASYNC
# ============================================================================

@pytest.fixture(scope="session")
def event_loop():
    """Event loop pour tests asyncio"""
    loop = asyncio.new_event_loop()
    yield loop
    loop.close()

# ============================================================================
# EXPORTS POUR TESTS
# ============================================================================

__all__ = [
    'create_test_questionnaire',
    'assert_score_valid', 
    'assert_processing_time_valid'
]

# Message de d√©marrage
print("‚úÖ Configuration Nextvision V3.0 - Conftest charg√© avec succ√®s")
