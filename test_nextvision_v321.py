#!/usr/bin/env python3
"""
ğŸ§ª SCRIPT DE TEST AUTOMATISÃ‰ - NEXTVISION v3.2.1
================================================

VALIDATION RÃ‰VOLUTION ARCHITECTURE : Tests automatisÃ©s complets
- Endpoint Intelligent v3 : Workflow unifiÃ© opÃ©rationnel
- Adaptateur Intelligent : Transformations format automatiques
- Performance : < 2000ms validation
- Architecture : Tous composants fonctionnels

Author: NEXTEN Team
Version: 3.2.1
Innovation: Tests complets workflow rÃ©volutionnaire
"""

import asyncio
import aiohttp
import json
import time
import tempfile
import os
import sys
from pathlib import Path
import logging
from typing import Dict, Any, List

# Configuration logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class NextvisionV3Tester:
    """
    ğŸ§ª TESTEUR AUTOMATISÃ‰ NEXTVISION v3.2.1
    ========================================
    
    **Mission** : Valider automatiquement toutes les innovations v3.2.1
    
    **Tests** :
    - âœ… API Health Checks complets
    - âœ… Endpoint Intelligent v3 fonctionnel 
    - âœ… Adaptateur Intelligent opÃ©rationnel
    - âœ… Workflow unifiÃ© automatique
    - âœ… Performance < 2000ms
    - âœ… Transport Intelligence intÃ©grÃ©
    
    **RÃ©sultat** : Validation complÃ¨te rÃ©volution architecture
    """
    
    def __init__(self, base_url: str = "http://localhost:8001"):
        self.base_url = base_url
        self.test_results = {
            "total_tests": 0,
            "passed": 0,
            "failed": 0,
            "performance_tests": [],
            "details": []
        }
        
    async def run_complete_tests(self) -> Dict[str, Any]:
        """ğŸš€ Lance tous les tests automatisÃ©s"""
        
        logger.info("ğŸ§ª === DÃ‰MARRAGE TESTS NEXTVISION v3.2.1 ===")
        start_time = time.time()
        
        # CrÃ©er session HTTP
        async with aiohttp.ClientSession() as session:
            self.session = session
            
            # === SUITE DE TESTS COMPLÃˆTE ===
            await self._test_api_health()
            await self._test_v3_endpoints()
            await self._test_intelligent_workflow()
            await self._test_adaptateur_direct()
            await self._test_performance_targets()
            await self._test_integration_complete()
            
        # Calcul rÃ©sultats finaux
        total_time = time.time() - start_time
        success_rate = (self.test_results["passed"] / max(1, self.test_results["total_tests"])) * 100
        
        final_report = {
            "status": "success" if success_rate >= 80 else "partial" if success_rate >= 60 else "failed",
            "summary": {
                "total_tests": self.test_results["total_tests"],
                "passed": self.test_results["passed"],
                "failed": self.test_results["failed"],
                "success_rate": round(success_rate, 1),
                "test_duration_seconds": round(total_time, 2)
            },
            "performance": {
                "tests_completed": len(self.test_results["performance_tests"]),
                "average_response_time": self._calculate_avg_performance(),
                "target_2000ms_achieved": all(t["time_ms"] < 2000 for t in self.test_results["performance_tests"])
            },
            "architecture_validation": {
                "api_health": any("health" in test["name"] and test["passed"] for test in self.test_results["details"]),
                "endpoint_v3": any("v3" in test["name"] and test["passed"] for test in self.test_results["details"]),
                "workflow_unifie": any("workflow" in test["name"] and test["passed"] for test in self.test_results["details"]),
                "adaptateur_intelligent": any("adaptateur" in test["name"] and test["passed"] for test in self.test_results["details"])
            },
            "detailed_results": self.test_results["details"],
            "recommendations": self._generate_recommendations()
        }
        
        # Affichage rÃ©sultats
        self._display_results(final_report)
        
        return final_report
    
    async def _test_api_health(self):
        """â¤ï¸ Test health checks API"""
        logger.info("ğŸ” Test API Health Checks...")
        
        health_endpoints = [
            "/api/v1/health",
            "/api/v3/health", 
            "/api/v3/status",
            "/api/v1/integration/health",
            "/api/v2/maps/health"
        ]
        
        for endpoint in health_endpoints:
            await self._test_endpoint_get(
                endpoint=endpoint,
                test_name=f"health_check_{endpoint.split('/')[-1]}",
                expected_status=200,
                required_fields=["status"]
            )
    
    async def _test_v3_endpoints(self):
        """ğŸ¯ Test endpoints v3 spÃ©cifiques"""
        logger.info("ğŸ¯ Test Endpoints v3...")
        
        # Test endpoint principal
        await self._test_endpoint_get(
            endpoint="/",
            test_name="root_endpoint_v3_integration",
            expected_status=200,
            required_fields=["version", "revolutionary_endpoint"]
        )
        
        # Test status dÃ©taillÃ© v3
        await self._test_endpoint_get(
            endpoint="/api/v3/status",
            test_name="v3_status_detailed",
            expected_status=200,
            required_fields=["version", "workflow", "services"]
        )
    
    async def _test_intelligent_workflow(self):
        """ğŸš€ Test workflow intelligent unifiÃ©"""
        logger.info("ğŸš€ Test Workflow Intelligent...")
        
        # CrÃ©er fichiers de test
        cv_content = """John Doe
john.doe@example.com
+33 6 12 34 56 78

COMPÃ‰TENCES:
- Python (5 ans)
- JavaScript (3 ans) 
- FastAPI (2 ans)
- React (2 ans)

EXPÃ‰RIENCE:
2019-2024: DÃ©veloppeur Senior chez TechCorp
- DÃ©veloppement APIs REST
- Architecture microservices
- Management Ã©quipe 3 personnes

FORMATION:
2017: Master Informatique, UniversitÃ© Paris-Saclay
"""
        
        job_content = """OFFRE D'EMPLOI: DÃ©veloppeur Full-Stack Senior
ENTREPRISE: InnovTech Solutions
LOCALISATION: Paris 15Ã¨me, France
CONTRAT: CDI

COMPÃ‰TENCES REQUISES:
- Python/FastAPI (4+ ans)
- JavaScript/React (3+ ans)
- Architecture API
- Leadership technique

MISSIONS:
- Conception architecture technique
- DÃ©veloppement applications web
- Encadrement Ã©quipe dÃ©veloppement
- Optimisation performance

SALAIRE: 55k-70kâ‚¬
TÃ‰LÃ‰TRAVAIL: 3j/semaine
"""
        
        # Test avec fichiers temporaires
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as cv_file:
            cv_file.write(cv_content)
            cv_file_path = cv_file.name
            
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as job_file:
            job_file.write(job_content)
            job_file_path = job_file.name
        
        try:
            await self._test_intelligent_matching_endpoint(cv_file_path, job_file_path)
        finally:
            # Nettoyage fichiers temporaires
            os.unlink(cv_file_path)
            os.unlink(job_file_path)
    
    async def _test_intelligent_matching_endpoint(self, cv_path: str, job_path: str):
        """ğŸ¯ Test endpoint intelligent matching"""
        
        start_time = time.time()
        
        try:
            # PrÃ©parer donnÃ©es multipart
            data = aiohttp.FormData()
            
            # Ajouter fichier CV
            with open(cv_path, 'rb') as cv_file:
                data.add_field('cv_file', cv_file, filename='test_cv.txt', content_type='text/plain')
                
                # Ajouter fichier Job
                with open(job_path, 'rb') as job_file:
                    data.add_field('job_file', job_file, filename='test_job.txt', content_type='text/plain')
                    
                    # Ajouter paramÃ¨tres
                    data.add_field('pourquoi_ecoute', 'Poste trop loin de mon domicile')
                    data.add_field('job_address', 'Paris 15Ã¨me, France')
                    
                    # Appel endpoint
                    async with self.session.post(
                        f"{self.base_url}/api/v3/intelligent-matching",
                        data=data
                    ) as response:
                        
                        response_time = (time.time() - start_time) * 1000
                        response_text = await response.text()
                        
                        # Validation rÃ©ponse
                        if response.status == 200:
                            try:
                                result = json.loads(response_text)
                                
                                # Validation structure rÃ©ponse
                                required_fields = [
                                    "status", "workflow", "matching_results", 
                                    "candidate_summary", "performance"
                                ]
                                
                                missing_fields = [field for field in required_fields if field not in result]
                                
                                if not missing_fields:
                                    # Test rÃ©ussi
                                    self._record_test_success(
                                        "intelligent_workflow_complete",
                                        f"Workflow unifiÃ© fonctionnel ({response_time:.0f}ms)",
                                        {"response_time": response_time, "result": result}
                                    )
                                    
                                    # Enregistrer performance
                                    self.test_results["performance_tests"].append({
                                        "test": "intelligent_matching",
                                        "time_ms": response_time,
                                        "target_achieved": response_time < 2000
                                    })
                                    
                                else:
                                    self._record_test_failure(
                                        "intelligent_workflow_structure",
                                        f"Champs manquants: {missing_fields}"
                                    )
                            except json.JSONDecodeError as e:
                                self._record_test_failure(
                                    "intelligent_workflow_json",
                                    f"RÃ©ponse JSON invalide: {str(e)}"
                                )
                        else:
                            self._record_test_failure(
                                "intelligent_workflow_http",
                                f"Status HTTP {response.status}: {response_text}"
                            )
        
        except Exception as e:
            self._record_test_failure(
                "intelligent_workflow_exception",
                f"Exception: {str(e)}"
            )
    
    async def _test_adaptateur_direct(self):
        """ğŸ”„ Test adaptateur intelligent directement"""
        logger.info("ğŸ”„ Test Adaptateur Intelligent...")
        
        try:
            # Import et test direct
            sys.path.insert(0, os.getcwd())
            
            from nextvision.adapters.parsing_to_matching_adapter import create_unified_matching_request
            
            # DonnÃ©es de test
            cv_data = {
                "name": "Alice Martin",
                "email": "alice.martin@example.com",
                "phone": "+33 6 12 34 56 78",
                "skills": ["Python", "React", "PostgreSQL", "Docker"],
                "years_of_experience": 4,
                "education": "Master Informatique",
                "location": "Lyon, France"
            }
            
            job_data = {
                "title": "DÃ©veloppeuse Full-Stack",
                "company": "TechStartup",
                "location": "Lyon, France",
                "required_skills": ["Python", "React", "Base de donnÃ©es"],
                "salary_range": {"min": 45000, "max": 55000},
                "contract_type": "CDI"
            }
            
            # Test transformation
            start_time = time.time()
            result = create_unified_matching_request(
                cv_data=cv_data,
                job_data=job_data,
                pourquoi_ecoute="RÃ©munÃ©ration trop faible"
            )
            adaptation_time = (time.time() - start_time) * 1000
            
            # Validation rÃ©sultat
            if result.success and result.matching_request:
                self._record_test_success(
                    "adaptateur_intelligent_direct",
                    f"Transformation rÃ©ussie ({adaptation_time:.0f}ms)",
                    {
                        "adaptations_applied": result.adaptations_applied,
                        "processing_time": result.processing_time_ms
                    }
                )
            else:
                self._record_test_failure(
                    "adaptateur_intelligent_validation",
                    f"Erreurs: {result.validation_errors}"
                )
                
        except ImportError as e:
            self._record_test_failure(
                "adaptateur_intelligent_import",
                f"Import failed: {str(e)}"
            )
        except Exception as e:
            self._record_test_failure(
                "adaptateur_intelligent_exception",
                f"Exception: {str(e)}"
            )
    
    async def _test_performance_targets(self):
        """âš¡ Test cibles de performance"""
        logger.info("âš¡ Test Performance Targets...")
        
        # Test multiple appels pour mesurer performance moyenne
        performance_tests = [
            ("/api/v1/health", "health_performance"),
            ("/api/v3/health", "v3_health_performance"), 
            ("/api/v3/status", "v3_status_performance")
        ]
        
        for endpoint, test_name in performance_tests:
            times = []
            
            # 3 appels pour moyenne
            for i in range(3):
                start_time = time.time()
                
                try:
                    async with self.session.get(f"{self.base_url}{endpoint}") as response:
                        await response.text()
                        response_time = (time.time() - start_time) * 1000
                        times.append(response_time)
                        
                except Exception:
                    times.append(5000)  # Timeout simulÃ©
            
            avg_time = sum(times) / len(times)
            
            # Validation cible performance
            if avg_time < 1000:  # < 1s pour endpoints simples
                self._record_test_success(
                    test_name,
                    f"Performance excellente ({avg_time:.0f}ms moyenne)"
                )
            else:
                self._record_test_failure(
                    test_name,
                    f"Performance lente ({avg_time:.0f}ms moyenne)"
                )
            
            # Enregistrer mÃ©trique
            self.test_results["performance_tests"].append({
                "test": test_name,
                "time_ms": avg_time,
                "target_achieved": avg_time < 1000
            })
    
    async def _test_integration_complete(self):
        """ğŸ”— Test intÃ©gration complÃ¨te"""
        logger.info("ğŸ”— Test IntÃ©gration ComplÃ¨te...")
        
        # Test cohÃ©rence version API
        try:
            async with self.session.get(f"{self.base_url}/") as response:
                if response.status == 200:
                    result = await response.json()
                    
                    # VÃ©rifier version 3.2.1
                    if result.get("version") == "3.2.1":
                        self._record_test_success(
                            "integration_version_consistency",
                            "Version 3.2.1 cohÃ©rente"
                        )
                    else:
                        self._record_test_failure(
                            "integration_version_mismatch",
                            f"Version incohÃ©rente: {result.get('version')}"
                        )
                    
                    # VÃ©rifier endpoint rÃ©volutionnaire
                    if "revolutionary_endpoint" in result:
                        endpoint_info = result["revolutionary_endpoint"]
                        if endpoint_info.get("url") == "/api/v3/intelligent-matching":
                            self._record_test_success(
                                "integration_revolutionary_endpoint",
                                "Endpoint rÃ©volutionnaire correctement rÃ©fÃ©rencÃ©"
                            )
                        else:
                            self._record_test_failure(
                                "integration_endpoint_config",
                                f"URL endpoint incorrecte: {endpoint_info.get('url')}"
                            )
        
        except Exception as e:
            self._record_test_failure(
                "integration_complete_exception",
                f"Exception intÃ©gration: {str(e)}"
            )
    
    async def _test_endpoint_get(self, endpoint: str, test_name: str, expected_status: int = 200, required_fields: List[str] = None):
        """ğŸŒ Test endpoint GET gÃ©nÃ©rique"""
        
        start_time = time.time()
        
        try:
            async with self.session.get(f"{self.base_url}{endpoint}") as response:
                response_time = (time.time() - start_time) * 1000
                
                if response.status == expected_status:
                    try:
                        result = await response.json()
                        
                        # VÃ©rifier champs requis
                        if required_fields:
                            missing_fields = [field for field in required_fields if field not in result]
                            
                            if missing_fields:
                                self._record_test_failure(
                                    test_name,
                                    f"Champs manquants: {missing_fields}"
                                )
                                return
                        
                        self._record_test_success(
                            test_name,
                            f"Endpoint fonctionnel ({response_time:.0f}ms)"
                        )
                        
                    except json.JSONDecodeError:
                        self._record_test_failure(
                            test_name,
                            "RÃ©ponse JSON invalide"
                        )
                else:
                    response_text = await response.text()
                    self._record_test_failure(
                        test_name,
                        f"Status HTTP {response.status}: {response_text[:200]}"
                    )
        
        except Exception as e:
            self._record_test_failure(
                test_name,
                f"Exception: {str(e)}"
            )
    
    def _record_test_success(self, test_name: str, message: str, details: Dict = None):
        """âœ… Enregistrer test rÃ©ussi"""
        self.test_results["total_tests"] += 1
        self.test_results["passed"] += 1
        self.test_results["details"].append({
            "name": test_name,
            "status": "PASSED",
            "message": message,
            "details": details,
            "passed": True
        })
        logger.info(f"âœ… {test_name}: {message}")
    
    def _record_test_failure(self, test_name: str, message: str):
        """âŒ Enregistrer test Ã©chouÃ©"""
        self.test_results["total_tests"] += 1
        self.test_results["failed"] += 1
        self.test_results["details"].append({
            "name": test_name,
            "status": "FAILED",
            "message": message,
            "passed": False
        })
        logger.error(f"âŒ {test_name}: {message}")
    
    def _calculate_avg_performance(self) -> float:
        """ğŸ“Š Calcule performance moyenne"""
        if not self.test_results["performance_tests"]:
            return 0.0
        
        total_time = sum(test["time_ms"] for test in self.test_results["performance_tests"])
        return round(total_time / len(self.test_results["performance_tests"]), 2)
    
    def _generate_recommendations(self) -> List[str]:
        """ğŸ’¡ GÃ©nÃ¨re recommandations"""
        recommendations = []
        
        success_rate = (self.test_results["passed"] / max(1, self.test_results["total_tests"])) * 100
        
        if success_rate < 80:
            recommendations.append("ğŸ”§ VÃ©rifier configuration API et dÃ©pendances")
        
        if self.test_results["failed"] > 0:
            recommendations.append("ğŸ“‹ Consulter logs dÃ©taillÃ©s pour erreurs spÃ©cifiques")
        
        avg_performance = self._calculate_avg_performance()
        if avg_performance > 2000:
            recommendations.append("âš¡ Optimiser performance - cible < 2000ms non atteinte")
        
        if success_rate >= 90:
            recommendations.append("ğŸš€ Excellent ! PrÃªt pour production")
        
        return recommendations
    
    def _display_results(self, report: Dict[str, Any]):
        """ğŸ“Š Affiche rÃ©sultats de test"""
        
        print("\n" + "="*80)
        print("ğŸ§ª RAPPORT DE TEST NEXTVISION v3.2.1")
        print("="*80)
        
        print(f"\nğŸ“Š RÃ‰SUMÃ‰:")
        print(f"   â€¢ Tests total: {report['summary']['total_tests']}")
        print(f"   â€¢ Tests rÃ©ussis: {report['summary']['passed']} âœ…")
        print(f"   â€¢ Tests Ã©chouÃ©s: {report['summary']['failed']} âŒ")
        print(f"   â€¢ Taux de succÃ¨s: {report['summary']['success_rate']}%")
        print(f"   â€¢ DurÃ©e totale: {report['summary']['test_duration_seconds']}s")
        
        print(f"\nâš¡ PERFORMANCE:")
        print(f"   â€¢ Tests performance: {report['performance']['tests_completed']}")
        print(f"   â€¢ Temps moyen: {report['performance']['average_response_time']}ms")
        print(f"   â€¢ Cible < 2000ms: {'âœ… ATTEINTE' if report['performance']['target_2000ms_achieved'] else 'âŒ NON ATTEINTE'}")
        
        print(f"\nğŸ—ï¸ VALIDATION ARCHITECTURE:")
        for component, status in report['architecture_validation'].items():
            status_icon = "âœ…" if status else "âŒ"
            print(f"   â€¢ {component}: {status_icon}")
        
        if report['recommendations']:
            print(f"\nğŸ’¡ RECOMMANDATIONS:")
            for rec in report['recommendations']:
                print(f"   â€¢ {rec}")
        
        # Status final
        status_color = {
            "success": "ğŸŸ¢",
            "partial": "ğŸŸ¡", 
            "failed": "ğŸ”´"
        }
        
        print(f"\nğŸ¯ STATUT FINAL: {status_color.get(report['status'], 'âšª')} {report['status'].upper()}")
        
        if report['status'] == 'success':
            print("\nğŸš€ RÃ‰VOLUTION NEXTVISION v3.2.1 VALIDÃ‰E AVEC SUCCÃˆS !")
            print("   Architecture optimisÃ©e et workflow unifiÃ© opÃ©rationnels")
        
        print("="*80)

# === FONCTION PRINCIPALE ===

async def main():
    """ğŸš€ Fonction principale de test"""
    
    print("ğŸ§ª === NEXTVISION v3.2.1 TEST AUTOMATISÃ‰ ===")
    print("ğŸ¯ Validation complÃ¨te rÃ©volution architecture")
    print("=" * 50)
    
    # VÃ©rifier que l'API est dÃ©marrÃ©e
    print("\nâš ï¸  PRÃ‰REQUIS:")
    print("   â€¢ API Nextvision doit Ãªtre dÃ©marrÃ©e sur http://localhost:8001")
    print("   â€¢ Commande: python main.py")
    print("")
    
    response = input("ğŸ¤” L'API est-elle dÃ©marrÃ©e ? (y/N): ")
    if response.lower() != 'y':
        print("âŒ DÃ©marrer l'API avant de lancer les tests")
        print("   cd /Users/baptistecomas/Nextvision/")
        print("   source nextvision_env/bin/activate")
        print("   python main.py")
        return
    
    # Lancer tests
    tester = NextvisionV3Tester()
    
    try:
        report = await tester.run_complete_tests()
        
        # Sauvegarder rapport
        report_file = f"test_report_v321_{int(time.time())}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ Rapport sauvegardÃ©: {report_file}")
        
        # Exit code selon rÃ©sultat
        if report['status'] == 'success':
            sys.exit(0)
        elif report['status'] == 'partial':
            sys.exit(1)
        else:
            sys.exit(2)
            
    except KeyboardInterrupt:
        print("\nâŒ Tests interrompus par l'utilisateur")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ Erreur critique lors des tests: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
