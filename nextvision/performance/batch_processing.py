"""
‚ö° Batch Processing - Production Enterprise Grade

Optimisation traitement batch pour 1000+ jobs simultan√©s :
‚Ä¢ Traitement parall√®le intelligent
‚Ä¢ Pool de workers adaptatif
‚Ä¢ Queue management avec priorit√©s
‚Ä¢ Memory management optimis√©
‚Ä¢ Circuit breaker pour protection
‚Ä¢ Monitoring temps r√©el
‚Ä¢ Target: 1000 jobs < 2s

Author: NEXTEN Team - Production Robustness
Version: 1.0.0
"""

import asyncio
import time
import math
from datetime import datetime, timedelta
from enum import Enum
from typing import Dict, List, Optional, Any, Callable, Union, Tuple
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp

from ..logging.structured_logging import get_structured_logger
from ..monitoring.health_metrics import HealthMetrics
from ..cache.redis_intelligent_cache import get_cache_instance, CacheKey

logger = get_structured_logger(__name__)
health_metrics = HealthMetrics()

class BatchPriority(Enum):
    """üéØ Priorit√©s de traitement batch"""
    CRITICAL = 1
    HIGH = 2
    NORMAL = 3
    LOW = 4
    BACKGROUND = 5

class ProcessingMode(Enum):
    """üîß Modes de traitement"""
    ASYNC_ONLY = "async_only"        # Asyncio seulement
    THREAD_POOL = "thread_pool"      # ThreadPoolExecutor
    PROCESS_POOL = "process_pool"    # ProcessPoolExecutor
    HYBRID = "hybrid"                # Mix adaptatif

@dataclass
class BatchConfig:
    """‚öôÔ∏è Configuration optimis√©e traitement batch"""
    # Performance Targets
    target_jobs_per_second: int = 500  # 1000 jobs < 2s = 500 jobs/s
    max_concurrent_jobs: int = 100
    max_memory_mb: int = 512
    
    # Worker Management
    min_workers: int = 4
    max_workers: int = 16
    worker_auto_scale: bool = True
    worker_idle_timeout: int = 30
    
    # Queue Management
    max_queue_size: int = 10000
    priority_enabled: bool = True
    queue_timeout_seconds: int = 60
    
    # Processing Modes
    default_mode: ProcessingMode = ProcessingMode.HYBRID
    cpu_intensive_threshold: float = 0.1  # secondes
    use_process_pool_threshold: int = 50  # nb jobs
    
    # Memory Management
    chunk_size: int = 50  # Jobs par chunk
    memory_pressure_threshold: float = 0.8  # 80% utilisation
    gc_frequency: int = 100  # GC tous les N jobs
    
    # Monitoring & Retries
    enable_monitoring: bool = True
    retry_failed_jobs: bool = True
    max_retries: int = 3
    retry_delay_seconds: float = 1.0
    
    # Cache Integration
    enable_result_caching: bool = True
    cache_ttl_seconds: int = 3600  # 1h
    
    # Circuit Breaker
    failure_threshold: int = 10
    circuit_timeout_seconds: int = 60

@dataclass
class JobBatch:
    """üì¶ Batch de jobs √† traiter"""
    jobs: List[Dict[str, Any]]
    batch_id: str
    priority: BatchPriority = BatchPriority.NORMAL
    processing_mode: Optional[ProcessingMode] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)
    
    def __len__(self) -> int:
        return len(self.jobs)
    
    def estimate_processing_time(self, jobs_per_second: float) -> float:
        """‚è±Ô∏è Estimation temps de traitement"""
        return len(self.jobs) / jobs_per_second if jobs_per_second > 0 else float('inf')

@dataclass
class BatchResult:
    """üìä R√©sultat traitement batch"""
    batch_id: str
    total_jobs: int
    successful_jobs: int
    failed_jobs: int
    
    # Performance Metrics
    processing_time_seconds: float
    jobs_per_second: float
    
    # Results
    results: List[Any]
    errors: List[Tuple[int, Exception]]  # (job_index, error)
    
    # Metadata
    processing_mode_used: ProcessingMode
    workers_used: int
    memory_peak_mb: float
    cache_hits: int = 0
    
    started_at: datetime
    completed_at: datetime
    
    @property
    def success_rate(self) -> float:
        """üìà Taux de succ√®s"""
        return (self.successful_jobs / self.total_jobs * 100) if self.total_jobs > 0 else 0.0
    
    @property
    def performance_rating(self) -> str:
        """‚≠ê Rating performance"""
        if self.jobs_per_second >= 500:
            return "EXCELLENT"
        elif self.jobs_per_second >= 300:
            return "GOOD"
        elif self.jobs_per_second >= 100:
            return "AVERAGE"
        else:
            return "POOR"

class ParallelExecutor:
    """‚ö° Ex√©cuteur parall√®le adaptatif"""
    
    def __init__(self, config: BatchConfig):
        self.config = config
        self.thread_pool: Optional[ThreadPoolExecutor] = None
        self.process_pool: Optional[ProcessPoolExecutor] = None
        self.active_workers = 0
        self.peak_memory_mb = 0.0
        
        # Circuit Breaker
        self.failure_count = 0
        self.last_failure_time: Optional[datetime] = None
        self.circuit_open = False
        
        logger.info("‚ö° ParallelExecutor initialized", extra={
            "max_workers": config.max_workers,
            "default_mode": config.default_mode.value
        })
    
    async def execute_batch(self, batch: JobBatch, processor_func: Callable) -> BatchResult:
        """üöÄ Ex√©cution batch avec mode adaptatif"""
        start_time = time.time()
        started_at = datetime.now()
        
        logger.info(f"üöÄ Starting batch execution", extra={
            "batch_id": batch.batch_id,
            "job_count": len(batch.jobs),
            "priority": batch.priority.name,
            "estimated_time": batch.estimate_processing_time(self.config.target_jobs_per_second)
        })
        
        # V√©rification circuit breaker
        if self._is_circuit_open():
            raise Exception(f"Circuit breaker OPEN - too many failures")
        
        # S√©lection mode de traitement
        processing_mode = self._select_processing_mode(batch)
        workers_count = self._calculate_optimal_workers(batch)
        
        try:
            # Ex√©cution selon le mode
            if processing_mode == ProcessingMode.ASYNC_ONLY:
                results, errors, cache_hits = await self._execute_async(batch, processor_func, workers_count)
            elif processing_mode == ProcessingMode.THREAD_POOL:
                results, errors, cache_hits = await self._execute_thread_pool(batch, processor_func, workers_count)
            elif processing_mode == ProcessingMode.PROCESS_POOL:
                results, errors, cache_hits = await self._execute_process_pool(batch, processor_func, workers_count)
            else:  # HYBRID
                results, errors, cache_hits = await self._execute_hybrid(batch, processor_func, workers_count)
            
            # Calcul m√©triques
            processing_time = time.time() - start_time
            jobs_per_second = len(batch.jobs) / processing_time if processing_time > 0 else 0
            
            # Reset circuit breaker en cas de succ√®s
            self._reset_circuit_breaker()
            
            # R√©sultat
            result = BatchResult(
                batch_id=batch.batch_id,
                total_jobs=len(batch.jobs),
                successful_jobs=len(results) - len(errors),
                failed_jobs=len(errors),
                processing_time_seconds=processing_time,
                jobs_per_second=jobs_per_second,
                results=results,
                errors=errors,
                processing_mode_used=processing_mode,
                workers_used=workers_count,
                memory_peak_mb=self.peak_memory_mb,
                cache_hits=cache_hits,
                started_at=started_at,
                completed_at=datetime.now()
            )
            
            # Monitoring
            health_metrics.record_batch_completion(
                batch.batch_id, processing_time, jobs_per_second, result.success_rate
            )
            
            logger.info(f"‚úÖ Batch completed", extra={
                "batch_id": batch.batch_id,
                "processing_time_ms": round(processing_time * 1000, 2),
                "jobs_per_second": round(jobs_per_second, 1),
                "success_rate": round(result.success_rate, 1),
                "performance_rating": result.performance_rating,
                "workers_used": workers_count,
                "cache_hits": cache_hits
            })
            
            return result
            
        except Exception as e:
            self._handle_failure(e)
            processing_time = time.time() - start_time
            
            logger.error(f"‚ùå Batch execution failed", extra={
                "batch_id": batch.batch_id,
                "error": str(e)[:500],
                "processing_time_ms": round(processing_time * 1000, 2)
            })
            
            health_metrics.record_error("batch_processor", "execution_failed")
            raise e
    
    def _select_processing_mode(self, batch: JobBatch) -> ProcessingMode:
        """üéØ S√©lection mode de traitement optimal"""
        if batch.processing_mode:
            return batch.processing_mode
        
        job_count = len(batch.jobs)
        
        # Logique adaptative
        if job_count < 10:
            return ProcessingMode.ASYNC_ONLY
        elif job_count < self.config.use_process_pool_threshold:
            return ProcessingMode.THREAD_POOL
        elif job_count > 200:  # Gros batch
            return ProcessingMode.PROCESS_POOL
        else:
            return ProcessingMode.HYBRID
    
    def _calculate_optimal_workers(self, batch: JobBatch) -> int:
        """üë• Calcul nombre optimal de workers"""
        job_count = len(batch.jobs)
        
        # Base sur CPU cores
        cpu_cores = mp.cpu_count()
        
        # Calcul adaptatif
        if job_count <= 20:
            workers = min(job_count, self.config.min_workers)
        elif job_count <= 100:
            workers = min(cpu_cores, self.config.max_workers // 2)
        else:
            workers = min(self.config.max_workers, cpu_cores * 2)
        
        return max(self.config.min_workers, workers)
    
    async def _execute_async(self, batch: JobBatch, processor_func: Callable, workers: int) -> Tuple[List[Any], List[Tuple[int, Exception]], int]:
        """üîÑ Ex√©cution pure asyncio"""
        semaphore = asyncio.Semaphore(workers)
        cache = await get_cache_instance()
        
        results = [None] * len(batch.jobs)
        errors = []
        cache_hits = 0
        
        async def process_job(index: int, job: Dict[str, Any]):
            nonlocal cache_hits
            async with semaphore:
                try:
                    # V√©rification cache
                    if self.config.enable_result_caching:
                        cache_key = CacheKey.create_matching(
                            job.get("candidate_id", ""),
                            job.get("job_id", ""),
                            job.get("weights_hash", "")
                        )
                        cached_result = await cache.get(cache_key)
                        if cached_result is not None:
                            results[index] = cached_result
                            cache_hits += 1
                            return
                    
                    # Traitement
                    result = await processor_func(job)
                    results[index] = result
                    
                    # Mise en cache
                    if self.config.enable_result_caching:
                        await cache.set(cache_key, result, self.config.cache_ttl_seconds)
                    
                except Exception as e:
                    errors.append((index, e))
                    logger.warning(f"‚ö†Ô∏è Job {index} failed: {e}")
        
        # Ex√©cution parall√®le
        tasks = [process_job(i, job) for i, job in enumerate(batch.jobs)]
        await asyncio.gather(*tasks, return_exceptions=True)
        
        return results, errors, cache_hits
    
    async def _execute_thread_pool(self, batch: JobBatch, processor_func: Callable, workers: int) -> Tuple[List[Any], List[Tuple[int, Exception]], int]:
        """üßµ Ex√©cution avec ThreadPoolExecutor"""
        if not self.thread_pool or self.thread_pool._max_workers != workers:
            if self.thread_pool:
                self.thread_pool.shutdown(wait=False)
            self.thread_pool = ThreadPoolExecutor(max_workers=workers)
        
        loop = asyncio.get_event_loop()
        cache = await get_cache_instance()
        
        results = [None] * len(batch.jobs)
        errors = []
        cache_hits = 0
        
        def sync_processor(index: int, job: Dict[str, Any]):
            """üîÑ Processeur synchrone pour thread pool"""
            try:
                # Note: Cache check would need to be async, skipping for thread pool
                # In real implementation, you'd need async-to-sync bridge
                result = processor_func(job) if not asyncio.iscoroutinefunction(processor_func) else asyncio.run(processor_func(job))
                return index, result, None
            except Exception as e:
                return index, None, e
        
        # Soumission tasks
        futures = []
        for i, job in enumerate(batch.jobs):
            future = loop.run_in_executor(self.thread_pool, sync_processor, i, job)
            futures.append(future)
        
        # Attente r√©sultats
        completed_futures = await asyncio.gather(*futures, return_exceptions=True)
        
        for result in completed_futures:
            if isinstance(result, Exception):
                continue
            
            index, job_result, error = result
            if error:
                errors.append((index, error))
            else:
                results[index] = job_result
        
        return results, errors, cache_hits
    
    async def _execute_process_pool(self, batch: JobBatch, processor_func: Callable, workers: int) -> Tuple[List[Any], List[Tuple[int, Exception]], int]:
        """üîÑ Ex√©cution avec ProcessPoolExecutor"""
        if not self.process_pool or self.process_pool._max_workers != workers:
            if self.process_pool:
                self.process_pool.shutdown(wait=False)
            self.process_pool = ProcessPoolExecutor(max_workers=workers)
        
        loop = asyncio.get_event_loop()
        
        results = [None] * len(batch.jobs)
        errors = []
        cache_hits = 0  # Cache pas disponible en process pool
        
        def process_job_batch(job_chunk: List[Tuple[int, Dict[str, Any]]]):
            """üîÑ Traitement chunk de jobs"""
            chunk_results = []
            for index, job in job_chunk:
                try:
                    # Process job (fonction doit √™tre pickle-able)
                    result = processor_func(job)
                    chunk_results.append((index, result, None))
                except Exception as e:
                    chunk_results.append((index, None, e))
            return chunk_results
        
        # Division en chunks
        chunk_size = max(1, len(batch.jobs) // workers)
        job_chunks = []
        
        for i in range(0, len(batch.jobs), chunk_size):
            chunk = [(i + j, batch.jobs[i + j]) for j in range(min(chunk_size, len(batch.jobs) - i))]
            job_chunks.append(chunk)
        
        # Soumission chunks
        futures = []
        for chunk in job_chunks:
            future = loop.run_in_executor(self.process_pool, process_job_batch, chunk)
            futures.append(future)
        
        # Attente r√©sultats
        chunk_results = await asyncio.gather(*futures, return_exceptions=True)
        
        # Agr√©gation r√©sultats
        for chunk_result in chunk_results:
            if isinstance(chunk_result, Exception):
                continue
            
            for index, job_result, error in chunk_result:
                if error:
                    errors.append((index, error))
                else:
                    results[index] = job_result
        
        return results, errors, cache_hits
    
    async def _execute_hybrid(self, batch: JobBatch, processor_func: Callable, workers: int) -> Tuple[List[Any], List[Tuple[int, Exception]], int]:
        """üé≠ Ex√©cution hybride adaptative"""
        # Strat√©gie: async pour petits jobs, thread pool pour gros
        if len(batch.jobs) < 50:
            return await self._execute_async(batch, processor_func, workers)
        else:
            return await self._execute_thread_pool(batch, processor_func, workers)
    
    def _is_circuit_open(self) -> bool:
        """üîå V√©rification circuit breaker"""
        if not self.circuit_open:
            return False
        
        # V√©rifier si on peut r√©essayer
        if self.last_failure_time:
            elapsed = datetime.now() - self.last_failure_time
            if elapsed.total_seconds() > self.config.circuit_timeout_seconds:
                self.circuit_open = False
                self.failure_count = 0
                logger.info("üîÑ Circuit breaker reset")
                return False
        
        return True
    
    def _handle_failure(self, error: Exception):
        """‚ùå Gestion √©chec avec circuit breaker"""
        self.failure_count += 1
        self.last_failure_time = datetime.now()
        
        if self.failure_count >= self.config.failure_threshold:
            self.circuit_open = True
            logger.error(f"üö® Circuit breaker OPEN after {self.failure_count} failures")
    
    def _reset_circuit_breaker(self):
        """‚úÖ Reset circuit breaker apr√®s succ√®s"""
        if self.failure_count > 0:
            self.failure_count = 0
            self.circuit_open = False
    
    async def cleanup(self):
        """üßπ Nettoyage ressources"""
        try:
            if self.thread_pool:
                self.thread_pool.shutdown(wait=True)
            if self.process_pool:
                self.process_pool.shutdown(wait=True)
            
            logger.info("üßπ ParallelExecutor cleanup completed")
        except Exception as e:
            logger.error(f"‚ùå ParallelExecutor cleanup error: {e}")

class BatchProcessor:
    """‚ö° Processeur principal pour traitement batch optimis√©"""
    
    def __init__(self, config: BatchConfig):
        self.config = config
        self.executor = ParallelExecutor(config)
        self.processing_queue: asyncio.Queue = asyncio.Queue(maxsize=config.max_queue_size)
        self.results_cache: Dict[str, BatchResult] = {}
        
        # Monitoring
        self.processed_batches = 0
        self.total_jobs_processed = 0
        self.average_jobs_per_second = 0.0
        
        logger.info("‚ö° BatchProcessor initialized", extra={
            "target_jobs_per_second": config.target_jobs_per_second,
            "max_concurrent_jobs": config.max_concurrent_jobs,
            "max_workers": config.max_workers
        })
    
    async def process_batch(self, batch: JobBatch, processor_func: Callable) -> BatchResult:
        """üöÄ Traitement batch principal"""
        # Validation
        if len(batch.jobs) == 0:
            raise ValueError("Batch cannot be empty")
        
        if len(batch.jobs) > self.config.max_concurrent_jobs:
            # Division en sous-batches
            return await self._process_large_batch(batch, processor_func)
        
        # Traitement direct
        result = await self.executor.execute_batch(batch, processor_func)
        
        # Mise √† jour m√©triques
        self._update_metrics(result)
        
        # Cache r√©sultat
        if self.config.enable_result_caching:
            self.results_cache[batch.batch_id] = result
        
        return result
    
    async def _process_large_batch(self, batch: JobBatch, processor_func: Callable) -> BatchResult:
        """üì¶ Traitement batch volumineux par chunks"""
        chunk_size = self.config.max_concurrent_jobs
        sub_batches = []
        
        # Division en sous-batches
        for i in range(0, len(batch.jobs), chunk_size):
            chunk_jobs = batch.jobs[i:i + chunk_size]
            sub_batch = JobBatch(
                jobs=chunk_jobs,
                batch_id=f"{batch.batch_id}_chunk_{i // chunk_size}",
                priority=batch.priority,
                processing_mode=batch.processing_mode,
                metadata={**batch.metadata, "parent_batch": batch.batch_id}
            )
            sub_batches.append(sub_batch)
        
        logger.info(f"üì¶ Processing large batch in {len(sub_batches)} chunks", extra={
            "batch_id": batch.batch_id,
            "total_jobs": len(batch.jobs),
            "chunk_size": chunk_size
        })
        
        # Traitement parall√®le des sous-batches
        sub_results = []
        start_time = time.time()
        
        tasks = [self.executor.execute_batch(sub_batch, processor_func) for sub_batch in sub_batches]
        completed_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Agr√©gation r√©sultats
        total_successful = 0
        total_failed = 0
        all_results = []
        all_errors = []
        total_cache_hits = 0
        
        for result in completed_results:
            if isinstance(result, Exception):
                logger.error(f"‚ùå Sub-batch failed: {result}")
                continue
            
            total_successful += result.successful_jobs
            total_failed += result.failed_jobs
            all_results.extend(result.results)
            all_errors.extend(result.errors)
            total_cache_hits += result.cache_hits
        
        processing_time = time.time() - start_time
        jobs_per_second = len(batch.jobs) / processing_time if processing_time > 0 else 0
        
        # R√©sultat consolid√©
        consolidated_result = BatchResult(
            batch_id=batch.batch_id,
            total_jobs=len(batch.jobs),
            successful_jobs=total_successful,
            failed_jobs=total_failed,
            processing_time_seconds=processing_time,
            jobs_per_second=jobs_per_second,
            results=all_results,
            errors=all_errors,
            processing_mode_used=ProcessingMode.HYBRID,
            workers_used=self.config.max_workers,
            memory_peak_mb=max(r.memory_peak_mb for r in completed_results if isinstance(r, BatchResult)),
            cache_hits=total_cache_hits,
            started_at=batch.created_at,
            completed_at=datetime.now()
        )
        
        return consolidated_result
    
    def _update_metrics(self, result: BatchResult):
        """üìä Mise √† jour m√©triques performance"""
        self.processed_batches += 1
        self.total_jobs_processed += result.total_jobs
        
        # Moyenne mobile jobs/sec
        if self.average_jobs_per_second == 0:
            self.average_jobs_per_second = result.jobs_per_second
        else:
            self.average_jobs_per_second = (self.average_jobs_per_second * 0.8) + (result.jobs_per_second * 0.2)
    
    async def get_performance_stats(self) -> Dict:
        """üìä Statistiques performance"""
        return {
            "performance_summary": {
                "processed_batches": self.processed_batches,
                "total_jobs_processed": self.total_jobs_processed,
                "average_jobs_per_second": round(self.average_jobs_per_second, 1),
                "target_achievement": round((self.average_jobs_per_second / self.config.target_jobs_per_second) * 100, 1)
            },
            "configuration": {
                "target_jobs_per_second": self.config.target_jobs_per_second,
                "max_concurrent_jobs": self.config.max_concurrent_jobs,
                "max_workers": self.config.max_workers,
                "chunk_size": self.config.chunk_size
            },
            "queue_status": {
                "queue_size": self.processing_queue.qsize(),
                "max_queue_size": self.config.max_queue_size
            },
            "cache_status": {
                "cached_results": len(self.results_cache),
                "cache_enabled": self.config.enable_result_caching
            },
            "timestamp": datetime.now().isoformat()
        }
    
    async def cleanup(self):
        """üßπ Nettoyage ressources"""
        await self.executor.cleanup()
        self.results_cache.clear()
        logger.info("üßπ BatchProcessor cleanup completed")

# ===============================================
# üöÄ GLOBAL INSTANCE & UTILITIES
# ===============================================

_batch_processor: Optional[BatchProcessor] = None

async def get_batch_processor(config: Optional[BatchConfig] = None) -> BatchProcessor:
    """‚ö° R√©cup√®re l'instance globale du batch processor"""
    global _batch_processor
    
    if _batch_processor is None:
        if config is None:
            config = BatchConfig()  # Configuration par d√©faut
        
        _batch_processor = BatchProcessor(config)
        logger.info("üöÄ Global batch processor initialized")
    
    return _batch_processor

# Utility function pour traitement simple
async def process_jobs_batch(jobs: List[Dict[str, Any]], 
                           processor_func: Callable,
                           batch_id: Optional[str] = None,
                           priority: BatchPriority = BatchPriority.NORMAL) -> BatchResult:
    """‚ö° Traitement batch simple"""
    if not batch_id:
        batch_id = f"batch_{int(time.time())}"
    
    batch = JobBatch(
        jobs=jobs,
        batch_id=batch_id,
        priority=priority
    )
    
    processor = await get_batch_processor()
    return await processor.process_batch(batch, processor_func)

# D√©corateur pour traitement batch automatique
def batch_process(batch_size: int = 50, priority: BatchPriority = BatchPriority.NORMAL):
    """‚ö° D√©corateur pour traitement batch automatique"""
    def decorator(func):
        async def wrapper(jobs: List[Dict[str, Any]], *args, **kwargs):
            # Cr√©ation batch
            batch_id = f"auto_batch_{int(time.time())}"
            
            # Si peu de jobs, traitement direct
            if len(jobs) <= batch_size:
                batch = JobBatch(jobs=jobs, batch_id=batch_id, priority=priority)
                processor = await get_batch_processor()
                return await processor.process_batch(batch, func)
            
            # Division en batches
            results = []
            for i in range(0, len(jobs), batch_size):
                chunk_jobs = jobs[i:i + batch_size]
                batch = JobBatch(
                    jobs=chunk_jobs,
                    batch_id=f"{batch_id}_chunk_{i // batch_size}",
                    priority=priority
                )
                
                processor = await get_batch_processor()
                chunk_result = await processor.process_batch(batch, func)
                results.append(chunk_result)
            
            return results
        return wrapper
    return decorator
